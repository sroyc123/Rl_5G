{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9aqpREHVIpB"
   },
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "N8RtzsJEVIpG"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import matlab.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng=matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "1f-qFfjGVIpO"
   },
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "EPISODES = 1\n",
    "TRAIN_END = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "xEv2fsseVIpO"
   },
   "outputs": [],
   "source": [
    "#Hyper Parameters\n",
    "def discount_rate(): #Gamma\n",
    "    return 0.95\n",
    "\n",
    "def learning_rate(): #Alpha\n",
    "    return 0.001\n",
    "\n",
    "def batch_size(): #Size of the batch used in the experience replay\n",
    "    return 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "R=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "69YfUyM5VIpP"
   },
   "outputs": [],
   "source": [
    "class DeepQNetwork():\n",
    "    def __init__(self, states, actions, alpha, gamma, epsilon,epsilon_min, epsilon_decay):\n",
    "        self.nS = states\n",
    "        self.nA = actions\n",
    "        self.memory = deque([], maxlen=250000)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        #Explore/Exploit\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.model = self.build_model()\n",
    "        self.loss = []\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = keras.Sequential() \n",
    "        model.add(keras.layers.Dense(24, input_dim=self.nS, activation='relu')) \n",
    "        model.add(keras.layers.Dense(24, activation='relu')) \n",
    "        model.add(keras.layers.Dense(self.nA, activation='linear')) \n",
    "        model.compile(loss='mean_squared_error',optimizer=keras.optimizers.Adam(lr=self.alpha)) \n",
    "        return model\n",
    "    import math\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    def action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(0,3,(int(nA/3)))\n",
    "        action_vals = self.model.predict(state) \n",
    "        temp=np.array(action_vals[0]);\n",
    "        action_2d=np.reshape(temp, (int(nA/3), 3));\n",
    "        action_val=np.argmax(action_2d, axis=1);\n",
    "        return action_val\n",
    "\n",
    "    def test_action(self, state): \n",
    "        temp=np.array(action_vals[0]);\n",
    "        action_2d=np.reshape(temp, (int(nA/3), 3));\n",
    "        action_val=np.argmax(action_2d, axis=1);\n",
    "        return action_val\n",
    "\n",
    "    def store(self, state, action, reward, nstate, done):\n",
    "        self.memory.append( (state, action, reward, nstate, done) )\n",
    "\n",
    "    def experience_replay(self, batch_size):\n",
    "        minibatch = random.sample( self.memory, batch_size ) \n",
    "        x = []\n",
    "        y = []\n",
    "        np_array = np.array(minibatch)\n",
    "        st = np.zeros((0,self.nS)) \n",
    "        nst = np.zeros((0,self.nS))\n",
    "        for i in range(len(np_array)): \n",
    "            st = np.append( st, np_array[i,0], axis=0)\n",
    "            nst = np.append( nst, np_array[i,3], axis=0)\n",
    "        st_predict = self.model.predict(st)\n",
    "        nst_predict = self.model.predict(nst)\n",
    "        index = 0\n",
    "        for state, action, reward, nstate, done in minibatch:\n",
    "            x.append(state)\n",
    "            nst_action_predict_model = nst_predict[index]\n",
    "            nst_action_predict_model_2d= np.reshape(nst_action_predict_model, (int(nA/3), 3));\n",
    "            target=np.zeros(int(nA/3));\n",
    "            if done == True: \n",
    "                target = target + reward\n",
    "            else:   \n",
    "                target = reward + self.gamma * np.amax(nst_action_predict_model_2d, axis=1);\n",
    "            qtobeupdated= np.argmax(nst_action_predict_model_2d, axis=1);\n",
    "            target_f = st_predict[index]           \n",
    "            for i in range(int(nA/3)):\n",
    "              cnt=i*3+qtobeupdated[i];\n",
    "              target_f[cnt]=target[i];\n",
    "                \n",
    "            y.append(target_f)\n",
    "            index += 1\n",
    "\n",
    "        x_reshape = np.array(x).reshape(batch_size,self.nS)\n",
    "        y_reshape = np.array(y)\n",
    "        epoch_count = 1 \n",
    "        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n",
    "        for i in range(epoch_count):\n",
    "            self.loss.append( hist.history['loss'][i] )\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "4kb9C0n1VIpR"
   },
   "outputs": [],
   "source": [
    "#Create the agent\n",
    "nS = 10\n",
    "nA = 3*nS\n",
    "dqn = DeepQNetwork(nS, nA, learning_rate(), discount_rate(), 1, 0.001, 0.995 )\n",
    "\n",
    "batch_size = batch_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "BjNQD_N4RMb_"
   },
   "outputs": [],
   "source": [
    "basepower=1;\n",
    "powerratio=10;\n",
    "numberoflevels=3;\n",
    "maxpower=basepower* (powerratio ** (numberoflevels -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSlkjkP9VIpS",
    "outputId": "c9491d6a-072a-41a6-b2ad-346d82d236b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 130.5708869443878 [ 1.  1. 10. 10.  1.  1.  1. 10. 10. 10.]\n",
      "1 : 113.3239910549212 [  1.  10.  10.  10.   1.   1.  10. 100. 100.   1.]\n",
      "2 : 132.75224246072915 [  1.  10.  10. 100.   1.   1. 100.  10. 100.   1.]\n",
      "3 : 116.45150936261288 [ 10.   1.   1.  10.   1.   1.  10.   1. 100.   1.]\n",
      "4 : 119.34670704725957 [ 10.  10.   1. 100.   1.   1.  10.   1. 100.   1.]\n",
      "5 : 119.25320150812371 [  1. 100.   1. 100.  10.   1. 100.   1. 100.  10.]\n",
      "6 : 123.31972344263417 [  1.  10.   1.  10.   1.   1. 100.   1. 100.   1.]\n",
      "7 : 131.54718855958438 [ 10.   1.  10.  10.   1.   1. 100.  10. 100.  10.]\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "rewards = [] \n",
    "epsilons = [] \n",
    "TEST_Episodes = 0\n",
    "for e in range(EPISODES):\n",
    "    state = np.zeros(nS) \n",
    "    state=state+basepower;\n",
    "    nstate = np.zeros(nS) \n",
    "    tot_rewards = 0\n",
    "    for time in range(1000): \n",
    "        action_val = dqn.action(state)\n",
    "        state = np.reshape(state,nS);\n",
    "        nstate = np.reshape(nstate,nS);\n",
    "        nstate[action_val==2]=state[action_val==2]*powerratio;\n",
    "        nstate[action_val==0]=state[action_val==0]/powerratio;\n",
    "        nstate[action_val==1]=state[action_val==1];\n",
    "        nstate[nstate<basepower]=basepower;\n",
    "        nstate[nstate>maxpower]=maxpower;\n",
    "        \n",
    "        xnstate= matlab.double(nstate.tolist())\n",
    "        reward= eng.fxn(xnstate);\n",
    "        print(time,':',reward,nstate)\n",
    "        R.append(reward)\n",
    "        done =False\n",
    "        \n",
    "        tot_rewards += reward\n",
    "        state = np.reshape(state, [1, nS])\n",
    "        nstate = np.reshape(nstate, [1, nS])\n",
    "        dqn.store(state, action_val, reward, nstate, done) \n",
    "        state = nstate\n",
    "        if done or time == 1000:\n",
    "            rewards.append(tot_rewards)\n",
    "            epsilons.append(dqn.epsilon)\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
    "                  .format(e, EPISODES, tot_rewards, dqn.epsilon))\n",
    "            break\n",
    "        if len(dqn.memory) > batch_size:\n",
    "            dqn.experience_replay(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('Reward','w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "N9QBdbGLVIpV",
    "outputId": "46f26117-a37f-4077-a685-abdb1d40a431"
   },
   "outputs": [],
   "source": [
    "for e_test in range(TEST_Episodes):\n",
    "    state = np.zeros(nS) # Resize to store in memory to pass to .predict\n",
    "    #define start state here\n",
    "    nstate = np.zeros(nS) \n",
    "    tot_rewards = 0\n",
    "    for t_test in range(210):\n",
    "        # action = dqn.test_action(state)\n",
    "        # nstate, reward, done, _ = envCartPole.step(action)\n",
    "        # nstate = np.reshape( nstate, [1, nS])\n",
    "        action_val = dqn.action(state)\n",
    "\n",
    "        nstate[action_val==2]=state[action_val==2]*powerratio;\n",
    "        nstate[action_val==0]=state[action_val==0]/powerratio;\n",
    "        nstate[action_val==1]=state[action_val==1];\n",
    "        nstate[nstate<basepower]=basepower;\n",
    "        nstate[nstate>maxpower]=maxpower;\n",
    "        # action = dqn.action(state)\n",
    "        # nstate[action>=0.75]=state[action>=0.75]+1;\n",
    "        # nstate[action<=0.25]=state[action<=0.25]-1;\n",
    "        # nstate[(action>=0.25)|(action<=0.75)]=state[(action>=0.25)|(action<=0.75)];\n",
    "        # nstate[nstate<1]=1;\n",
    "        # nstate[nstate>4]=4;\n",
    "        #get reward from matlab here and send the next state\n",
    "        #nstate, reward, done, _ = envCartPole.step(action)\n",
    "        done =false\n",
    "        \n",
    "        tot_rewards += reward\n",
    "        #DON'T STORE ANYTHING DURING TESTING\n",
    "        state = nstate\n",
    "        #done: CartPole fell. \n",
    "        #t_test == 209: CartPole stayed upright\n",
    "        if done or t_test == 209: \n",
    "            rewards.append(tot_rewards)\n",
    "            epsilons.append(0) #We are doing full exploit\n",
    "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
    "                  .format(e_test, TEST_Episodes, tot_rewards, 0))\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "MtPjVKy0VIpW",
    "outputId": "236b7e58-bd14-4875-8ae4-d6f9842dd7e4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'envCartPole' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-087450751fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0menvCartPole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'envCartPole' is not defined"
     ]
    }
   ],
   "source": [
    "rolling_average = np.convolve(rewards, np.ones(100)/100)\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(rolling_average, color='black')\n",
    "plt.axhline(y=195, color='r', linestyle='-') #Solved Line\n",
    "#Scale Epsilon (0.001 - 1.0) to match reward (0 - 200) range\n",
    "eps_graph = [200*x for x in epsilons]\n",
    "plt.plot(eps_graph, color='g', linestyle='-')\n",
    "#Plot the line where TESTING begins\n",
    "plt.axvline(x=TRAIN_END, color='y', linestyle='-')\n",
    "plt.xlim( (0,EPISODES) )\n",
    "plt.ylim( (0,220) )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "envCartPole.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "UQamDQNwork.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
